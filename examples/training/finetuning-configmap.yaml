apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-train
data:
  train_code.py: |-
    from datasets import load_dataset
    from trl import SFTConfig, SFTTrainer
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from ray.train import ScalingConfig, DataConfig, get_context
    from ray.train.torch import TorchTrainer
    import ray.train.huggingface.transformers
    from accelerate import Accelerator


    def train_func(config):
        # Function to load the training dataset into the proper format for instruction fine-tuning
        def formatting_prompts_func(example):
            text = f"### Instruction: {example['instruction']}\n ### Response: {example['response']}"
            return [text]

        accelerator = Accelerator()
        tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-3.2-1B")
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
        training_args = SFTConfig(
            output_dir="/tmp/llama-finetune",
            max_seq_length=512,
            per_device_train_batch_size=config["train_batch_size"],
            gradient_accumulation_steps=4,
            optim="paged_adamw_8bit",
            save_steps=config["save_steps"],
            logging_steps=config["logging_steps"],
            learning_rate=config["learning_rate"],
            weight_decay=0.001,
            max_grad_norm=0.3,
            max_steps=config["max_steps"],
            warmup_ratio=0.03,
            group_by_length=True,
            lr_scheduler_type="constant",
            fp16=True,
            bf16=False,
        )

        model = AutoModelForCausalLM.from_pretrained(
            "NousResearch/Llama-3.2-1B",
            device_map={"": get_context().get_local_rank()} # needed for setting the right rank from ray
        )
        train_ds = load_dataset("databricks/databricks-dolly-15k", split="train")
        trainer = accelerator.prepare(
            SFTTrainer(
                model=model,
                processing_class=tokenizer,
                train_dataset=train_ds,
                formatting_func=formatting_prompts_func,
                args=training_args,
            )
        )

        trainer.train()

    if __name__ == "__main__":
        train_loop_config = {
            "train_batch_size": 1,
            "learning_rate": 2e-4,
            "max_steps": 100,
            "save_steps": 10,
            "logging_steps": 10,
        }
        scaling_config = ScalingConfig(num_workers=4,
                                       use_gpu=True)
        run_config = ray.train.RunConfig(storage_path="/tmp/training-output")
        trainer = TorchTrainer(
            train_loop_per_worker=train_func,
            dataset_config=DataConfig(datasets_to_split=["train"]),
            train_loop_config = train_loop_config,
            run_config = run_config,
            scaling_config = scaling_config
        )
        # train
        result = trainer.fit()
